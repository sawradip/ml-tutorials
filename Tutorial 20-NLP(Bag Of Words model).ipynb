{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial 20-NLP(Bag Of Words model).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP7lr4365xXGxsAKV9WVTV4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1yULruM-zKgz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593262050028,"user_tz":-360,"elapsed":883,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}}},"source":["# Importing the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"qU5Yxwbe0Yck","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593262171197,"user_tz":-360,"elapsed":1080,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}}},"source":["# Importing the dataset\n","dataset = pd.read_csv('Restaurant_Reviews.tsv' , delimiter = '\\t' , quoting = 3)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WWtjiaqo2fOj","colab_type":"text"},"source":["Here We are going to use the 'Bag of Words'(BOW) method."]},{"cell_type":"code","metadata":{"id":"Vq5DgUiLEqLv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593266325118,"user_tz":-360,"elapsed":815,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}},"outputId":"8c0fbece-4fc2-4a58-f23b-cd53dcad388e"},"source":["len(dataset)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"7s0om0gq01_J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593267418650,"user_tz":-360,"elapsed":2713,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}},"outputId":"f2cd0bd4-806b-4977-dae2-3447e6f1c762"},"source":["#Cleaning the texts\n","import re                                                                                         #This library has the tools to clean dataset\n","import nltk\n","nltk.download('stopwords')                                                                        #This package contains all the stopwords(irrelivent words to ourcon text)\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer                                                        #This package will convert all the words to their root form\n","corpus = []\n","for i in range(len(dataset)):                                             \n","  review = re.sub('[^a-zA-Z]' , ' ' , dataset['Review'][i])                                       #We want to remove everything other than the characters,and remove all of them with spaces(' ')    \n","  review = review.lower()\n","  review = review.split()                                                                         #This will convert the text into list of words \n","  ps = PorterStemmer()\n","  review = [ ps.stem(word) for word in review if not word in set(stopwords.words('english'))]     #set function is must for  large text.For small texts like us,it is optional.                \n","  review = ' '.join(review)                                                                       #This will join the processed list of words into a sentence seperated by space(' ')\n","  corpus.append(review)                                                            "],"execution_count":27,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KpHgKEcN033a","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593267654784,"user_tz":-360,"elapsed":968,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}}},"source":["#Creating the Bag Of Words model\n","from sklearn.feature_extraction.text import  CountVectorizer\n","cv = CountVectorizer(max_features = 1500)\n","X = cv.fit_transform(corpus).toarray()\n","y = dataset.iloc[:,1].values"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wu-5puc-4lCp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593268014011,"user_tz":-360,"elapsed":966,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}},"outputId":"f1c05b42-78ca-4f08-b5f6-02d6de66236b"},"source":["#Splitting the dataset into Training set and Test set\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.1 , random_state = 0)\n","print(X_train.shape)\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["(900, 1500)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B4zXxhytI6O9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593268035262,"user_tz":-360,"elapsed":1231,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}},"outputId":"0989bc5d-4e74-4fea-de23-b99b4dcd19da"},"source":["#Fitting Naive Bayes Classifier to the dataset\n","\n","from sklearn.naive_bayes import GaussianNB\n","\n","classifier = GaussianNB()\n","classifier.fit(X_train,y_train)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GaussianNB(priors=None, var_smoothing=1e-09)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"s-0zqbOYLNmS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593268046495,"user_tz":-360,"elapsed":851,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}},"outputId":"8a048287-7ad1-409b-90d2-2cbb5b9a2962"},"source":["#Prediicting the Test Set results\n","\n","y_pred = classifier.predict(X_test)\n","print(y_pred)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["[1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1\n"," 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n"," 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"huOZnkQhLQcF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593268057444,"user_tz":-360,"elapsed":1008,"user":{"displayName":"Sawradip Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiknncyResq9DxB1krpJnhEFgTIz65ifrUq64R2hs=s64","userId":"07399611378604694216"}},"outputId":"dc3ae99e-7415-478c-9c23-7c53f2f81952"},"source":["#Making Confusion Matrix\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(y_test , y_pred)\n","print(cm)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["[[27 24]\n"," [ 3 46]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v_c420VYLTEl","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}